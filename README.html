<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="ai">#2048 AI</h2>
<h2 id="description">Description</h2>
<p><strong>Actions:</strong></p>
<ul>
<li>left/right/down/up</li>
</ul>
<p><strong>States:</strong><br />
n*m (i.e. 16 for 4X4 grid)</p>
<p><strong>Parameters:</strong></p>
<ul>
<li>Network:
<ul>
<li>Topology(#layers,neurons), @ Agent.h</li>
<li>Learning Rate, @ Agent.h</li>
<li>Weight Decay, @ Agent.h</li>
</ul></li>
<li>SARSA:
<ul>
<li>Learning Rate(alpha), @ GameManager.h</li>
<li>Discount Rate for future rewards(gamma) @ GameManager.h</li>
<li>Random Selection Rate(epsilon) @ Agent.h</li>
</ul></li>
<li>Epoch
<ul>
<li>Command Line</li>
</ul></li>
<li>Normalizing reward
<ul>
<li>Normalization: @ GameManager.h
<ul>
<li>r /= 2048.0</li>
<li>r /= maxR</li>
<li>r = 1 - 1/r</li>
</ul></li>
<li>Terminal State: @ GameManager.h
<ul>
<li>-1</li>
<li>0</li>
</ul></li>
</ul></li>
</ul>
<h2 id="progress">Progress</h2>
<ul>
<li>[x] Board Complete</li>
<li>[x] Game Logic Complete (for verification)</li>
<li>[x] Q-Learning Agent Implementation -- faulty</li>
<li>[x] Experience Replay</li>
<li>[ ] ConvNet (if needed)</li>
<li>[x] SARSA or Off-Policy Q-Learning? --&gt; Hybrid(Initially random-exploration, going off to SARSA)</li>
<li>[ ] SIGINT Handling to stop training &amp; view result</li>
<li>[x] Save/Load Trained Network</li>
<li>[x] Debugging Premature Capping Problem</li>
<li>[x] Better Determination of Terminal State</li>
<li>[x] Replace Deterministic Max Q-Value Exploration with Probabilistic Exploration</li>
<li>[x] Debug Neural Network : Back Propagation Doesn't seem to occur effectively.
<ul>
<li>Neural Network seems to be doing fine, simply a numerical instability for small numbers.</li>
</ul></li>
<li>[x] <del>Fix Game Logic Bug : Jumping Across Blocks</del>
<ul>
<li>Was Running Old Code</li>
</ul></li>
<li>[x] Converting vectors into templates (since they are fixed-size)</li>
<li>[x] Change Neural Net to Output Q-value for 4 actions as outputs
<ul>
<li>(i.e. Q(S) --(net)--&gt; {Q(S,a1),Q(S,a2)...})</li>
</ul></li>
<li>[x] Implement RMSProp to automatically adjust learning rate
<ul>
<li>Decided to use AdaDelta instead</li>
</ul></li>
</ul>
<hr />
<h2 id="notes">Notes</h2>
<p>At this point, I will implement the Agent using a Deep Neural Network<br />
with a simple multilayer construct.<br />
If that doesn't suffice, I will implement the Agent with a Convolutional Network.<br />
Given that the state space is anticipated to be enormous, it is impractical to use a Q-table.</p>
<p>I speculate that the Net doesn't learn very well because the changes in reward that occur due to a &quot;better&quot; move is very small (in the order of 1e-2); a better normalization scheme would be necessary. But how?</p>
<p>--</p>
<h2 id="results">Results</h2>
<p><img src="images/scores_avg_30000.png" alt="30000" /><br />
This is the training progress during 30000 iterations of training;</p>
<p><img src="images/scores_avg_100000.png" alt="100000" /><br />
And this is the training progress over 100000 games played.<br />
As seen, the average score gets higher and higher by the number of training samples!<br />
to put in context, a completely random agent achieves ~700 points per round.</p>
</body>
</html>
